{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOKnNSACYU0-"
   },
   "source": [
    "# LGU+ 경진대회 - 베이스라인  \n",
    "- [Neural Collaborative Filtering(NCF)](https://arxiv.org/pdf/1708.05031.pdf) 논문의 NeuMF를 참고하여 side-information을 결합한 모델을 PyTorch로 구현\n",
    "- 구현된 모델의 검증 데이터셋과 리더보드의 성능을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차 \n",
    "- 데이터 전처리 \n",
    "    - 기본 설정\n",
    "    - 데이터 불러오기 \n",
    "    - 학습 및 검증 데이터 생성 \n",
    "- NeuMF 구현    \n",
    "    - 모델 구현 \n",
    "    - 학습 및 추론 코드 구현\n",
    "- 모델 학습 \n",
    "    - 하이퍼파라미터 설정 & 최적화 기법 설정\n",
    "    - 모델 학습 \n",
    "    - 학습 과정 시각화 \n",
    "- 제출 \n",
    "    - 모든 유저에 대해 추천 결과 생성\n",
    "    - 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07lHW5CAYU1F"
   },
   "source": [
    "## 데이터 전처리\n",
    "### 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iHstgRfiYU1G"
   },
   "outputs": [],
   "source": [
    "# 패키지 로드\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os, random\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import plotnine\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 \n",
    "class cfg: \n",
    "    gpu_idx = 0\n",
    "    device = torch.device(\"cuda:{}\".format(gpu_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "    top_k = 25\n",
    "    seed = 42\n",
    "    neg_ratio = 100\n",
    "    test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5GrHkU7AYU1I"
   },
   "outputs": [],
   "source": [
    "# 시드 고정 \n",
    "def seed_everything(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "seed_everything(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "data_path = '../data'\n",
    "saved_path = './saved'\n",
    "output_path = './submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huw6BwpTYU1J"
   },
   "source": [
    "### 데이터 불러오기\n",
    "- history_data : 시청 시작 데이터\n",
    "- profile_data : 프로필 정보 \n",
    "- meta_data : 콘텐츠 일반 메타 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A23qw5LXYU1M"
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기 \n",
    "history_df = pd.read_csv(os.path.join(data_path, 'history_data.csv'), encoding='utf-8')\n",
    "profile_df = pd.read_csv(os.path.join(data_path, 'profile_data.csv'), encoding='utf-8')\n",
    "meta_df = pd.read_csv(os.path.join(data_path, 'meta_data.csv'), encoding='utf-8')\n",
    "\n",
    "search_df = pd.read_csv(os.path.join(data_path, 'search_data.csv'), encoding='utf-8')\n",
    "buy_df = pd.read_csv(os.path.join(data_path, 'buy_data.csv'), encoding='utf-8')\n",
    "meta_plus_df = pd.read_csv(os.path.join(data_path, 'meta_data_plus.csv'), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRnh1wEiYU1M"
   },
   "source": [
    "### 학습 및 검증 데이터 생성 \n",
    "- train : 시청 이력의 80%를 사용 \n",
    "- valid : 시청 이력의 20%를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 (중복제거) \n",
    "# 참고 : drop_duplicates의 subset을 무엇으로 구성하냐에 따라서 제거되는 항목들이 다름 \n",
    "# ex) 'profile_id', 'album_id' : 중복된 시청이력 모두 제거 / 'profile_id', 'album_id', 'log_time' : 같은 시간에 시청한 이력만 제거 \n",
    "data = history_df[['profile_id', 'log_time', 'album_id']].drop_duplicates(subset=['profile_id', 'album_id', 'log_time']).sort_values(by = ['profile_id', 'log_time']).reset_index(drop = True)\n",
    "data['rating'] = 1\n",
    "\n",
    "cfg.n_users = data.profile_id.max()+1\n",
    "cfg.n_items = data.album_id.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_id</th>\n",
       "      <th>log_time</th>\n",
       "      <th>album_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>search</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115719</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115809</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301115958</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301120118</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>20220301120229</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899247</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155668</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899248</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155680</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899249</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155810</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899250</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155838</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899251</th>\n",
       "      <td>33032</td>\n",
       "      <td>20220427155891</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899252 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        profile_id        log_time  album_id  rating  search  buy\n",
       "0                3  20220301115719        15       1       0    0\n",
       "1                3  20220301115809        16       1       0    0\n",
       "2                3  20220301115958        17       1       0    0\n",
       "3                3  20220301120118        18       1       0    0\n",
       "4                3  20220301120229        19       1       0    0\n",
       "...            ...             ...       ...     ...     ...  ...\n",
       "899247       33032  20220427155668       381       1       0    0\n",
       "899248       33032  20220427155680       381       1       0    0\n",
       "899249       33032  20220427155810       125       1       0    0\n",
       "899250       33032  20220427155838       125       1       0    0\n",
       "899251       33032  20220427155891       381       1       0    0\n",
       "\n",
       "[899252 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data에 search, buy 추가\n",
    "\n",
    "# search\n",
    "search_data = search_df[['profile_id', 'log_time', 'album_id']].drop_duplicates(subset=['profile_id', 'album_id', 'log_time']).sort_values(by=['profile_id', 'log_time']).reset_index(drop=True)\n",
    "data['search'] = 0\n",
    "\n",
    "for idx in search_data.index:\n",
    "    p, l, a = search_data.loc[idx, 'profile_id'], search_data.loc[idx, 'log_time']//1000000, search_data.loc[idx, 'album_id']\n",
    "    i = data[(data['profile_id'] == p) & (data['album_id']==a) & (data['log_time']//1000000 == l)].index\n",
    "    data.loc[i, 'search'] = 1\n",
    "\n",
    "# buy\n",
    "buy_data = buy_df[['profile_id', 'log_time', 'album_id']].drop_duplicates(subset=['profile_id', 'album_id', 'log_time']).sort_values(by=['profile_id', 'log_time']).reset_index(drop=True)\n",
    "data['buy'] = 0\n",
    "\n",
    "for idx in buy_data.index:\n",
    "    p, l, a = buy_data.loc[idx, 'profile_id'], buy_data.loc[idx, 'log_time']//1000000, buy_data.loc[idx, 'album_id']\n",
    "    i = data[(data['profile_id'] == p) & (data['album_id']==a) & (data['log_time']//1000000 == l)].index\n",
    "    data.loc[i, 'buy'] = 1\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 크기: (719401, 6)\n",
      "검증 데이터 크기: (179851, 6)\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 검증 데이터 분리\n",
    "train, valid = train_test_split(\n",
    "    data, test_size=cfg.test_size, random_state=cfg.seed,\n",
    ")\n",
    "print('학습 데이터 크기:', train.shape)\n",
    "print('검증 데이터 크기:', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7532a7408beb4196ad9c1246459900e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 형태: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix 형태로 변환 \n",
    "train = train.to_numpy()\n",
    "matrix = sparse.lil_matrix((cfg.n_users, cfg.n_items))\n",
    "for (p, _, i, r, _, _) in tqdm(train):\n",
    "    matrix[p, i] = r\n",
    "    \n",
    "train = sparse.csr_matrix(matrix)\n",
    "train = train.toarray()\n",
    "print(\"train 형태: \\n\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id 3의 age 정보 : 5\n"
     ]
    }
   ],
   "source": [
    "# 유저 특징 정보 추출 \n",
    "profile_df = profile_df.set_index('profile_id')\n",
    "user_features = profile_df[['age']].to_dict()\n",
    "print(\"user_id 3의 age 정보 :\", user_features['age'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_id 12의 관심키워드 정보 : [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 유저 특징 정보에 관심 키워드 추가\n",
    "\n",
    "interest_keyword_dict = {'P01': 0, 'P02': 1, 'P03': 2, 'P04': 3, 'P05': 4, 'P06': 5, 'P07': 6, 'P08': 7,\n",
    "                         'K01': 8, 'K02': 9, 'K03': 10, 'K04': 11, 'K05': 12, 'K06': 13, 'K07': 14, 'K08': 15, 'K09': 16}\n",
    "\n",
    "user_features['keyword'] = {}\n",
    "\n",
    "for p_id in profile_df.index:\n",
    "    interest_keywords = list(profile_df.loc[p_id, ['pr_interest_keyword_cd_1', 'pr_interest_keyword_cd_2', 'pr_interest_keyword_cd_3',\n",
    "                                                   'ch_interest_keyword_cd_1', 'ch_interest_keyword_cd_2', 'ch_interest_keyword_cd_3']])\n",
    "    \n",
    "    interest_keyword_vector = [0] * len(interest_keyword_dict)\n",
    "    for keyword in interest_keywords:\n",
    "        if keyword in interest_keyword_dict.keys():\n",
    "            idx = interest_keyword_dict[keyword]\n",
    "            interest_keyword_vector[idx] = 1\n",
    "        \n",
    "    user_features['keyword'][p_id] = interest_keyword_vector\n",
    "    \n",
    "print(\"profile_id 12의 관심키워드 정보 :\", user_features['keyword'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album_id 749의 genre_mid 정보 : 1\n"
     ]
    }
   ],
   "source": [
    "# 아이템 특징 정보 추출 \n",
    "meta_df = meta_df.set_index('album_id')\n",
    "\n",
    "# 범주형 데이터를 수치형 데이터로 변경 \n",
    "le = LabelEncoder()\n",
    "meta_df['genre_mid'] = le.fit_transform(meta_df['genre_mid'])\n",
    "item_features = meta_df[['genre_mid']].to_dict()\n",
    "print(\"album_id 749의 genre_mid 정보 :\", item_features['genre_mid'][749])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album_id 749의 tag 정보 : [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 아이템 특징 정보에 태그 추가\n",
    "meta_plus_data = meta_plus_df.set_index('album_id')\n",
    "meta_plus_data = meta_plus_data[meta_plus_data['keyword_value']==5]\n",
    "\n",
    "tag = meta_plus_data['keyword_type'].unique()\n",
    "tag_dict = dict(zip(tag, list(range(len(tag)))))\n",
    "\n",
    "item_features['tag'] = dict()\n",
    "\n",
    "for item_id in meta_df.index:\n",
    "    tag_vector = [0] * len(tag_dict)\n",
    "    \n",
    "    if item_id in meta_plus_data.index:\n",
    "        tags = list(meta_plus_data.loc[item_id,'keyword_type'])\n",
    "\n",
    "        for tag in tags:\n",
    "            if tag in tag_dict.keys():\n",
    "                idx = tag_dict[tag]\n",
    "                tag_vector[idx] = 1\n",
    "\n",
    "    item_features['tag'][item_id] = tag_vector\n",
    "    \n",
    "print(\"album_id 749의 tag 정보 :\", item_features['tag'][749])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출한 특징 정보의 속성을 저장 \n",
    "cfg.n_genres = meta_df['genre_mid'].nunique()\n",
    "cfg.n_keywords = len(interest_keyword_dict)\n",
    "cfg.n_tags = len(tag_dict)\n",
    "cfg.n_continuous_feats = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REzvMyWZYU1S"
   },
   "source": [
    "## NeuMF 구현\n",
    "\n",
    "\n",
    "### 모델 구현 \n",
    "- [Neural Collaborative Filtering(NCF)](https://arxiv.org/pdf/1708.05031.pdf) 논문의 NeuMF를 참고하여 side-information을 결합한 모델을 PyTorch로 구현\n",
    "    - continuous feature (age)와 categorical feature (genre_mid)를 같이 학습할 수 있도록 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://drive.google.com/uc?export=view&id=1tpajTLipLoFdvLICO-alAxeoKAE8-k61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hWOt2J5nYU1U"
   },
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \"\"\"Neural Matrix Factorization Model\n",
    "        참고 문헌 : https://arxiv.org/abs/1708.05031\n",
    "\n",
    "    예시 :\n",
    "        model = NeuMF(cfg) \n",
    "        output = model.forward(user_ids, item_ids, [feat0, feat1]) \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            cfg : config 파일로 네트워크 생성에 필요한 정보들을 담고 있음 \n",
    "        \"\"\"\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.n_users = cfg.n_users\n",
    "        self.n_items = cfg.n_items\n",
    "        self.emb_dim = cfg.emb_dim\n",
    "        self.layer_dim = cfg.layer_dim\n",
    "        self.n_continuous_feats = cfg.n_continuous_feats\n",
    "        self.n_genres = cfg.n_genres\n",
    "        self.n_keywords = cfg.n_keywords\n",
    "        self.n_tags = cfg.n_tags\n",
    "        self.dropout = cfg.dropout\n",
    "        self.build_graph()\n",
    "    \n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"Neural Matrix Factorization Model 생성\n",
    "            구현된 모습은 위의 그림을 참고 \n",
    "        \"\"\"\n",
    "        self.user_embedding_mf = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mf = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "        \n",
    "        self.user_embedding_mlp = nn.Embedding(num_embeddings=self.n_users, embedding_dim=self.emb_dim)\n",
    "        self.item_embedding_mlp = nn.Embedding(num_embeddings=self.n_items, embedding_dim=self.emb_dim)\n",
    "                \n",
    "        self.genre_embeddig = nn.Embedding(num_embeddings=self.n_genres, embedding_dim=self.n_genres//2)\n",
    "        self.keyword_embedding = nn.Embedding(num_embeddings=self.n_keywords, embedding_dim=self.n_keywords//2)\n",
    "        self.tag_embedding = nn.Embedding(num_embeddings=self.n_tags, embedding_dim=self.n_tags//2)\n",
    "        \n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim + self.n_genres//2 + self.n_keywords//2 + self.n_tags//2 + self.n_continuous_feats, self.layer_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout), \n",
    "            nn.Linear(self.layer_dim, self.layer_dim//2), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(p=self.dropout)\n",
    "        )\n",
    "        self.affine_output = nn.Linear(self.layer_dim//2 + self.emb_dim, 1)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            normal_(module.weight.data, mean=0.0, std=0.01)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            normal_(module.weight.data, 0, 0.01)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices, feats):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            user_indices : 유저의 인덱스 정보 \n",
    "                ex) tensor([ 3100,  3100,  ..., 14195, 14195])\n",
    "            item_indices : 아이템의 인덱스 정보\n",
    "                ex) tensor([   50,    65,   ..., 14960, 11527])\n",
    "            feats : 특징 정보 \n",
    "                    feats[0] : user age\n",
    "                    feats[1] : item genre\n",
    "                    feats[2] : user keyword\n",
    "                    feats[3] : item tag\n",
    "        Returns: \n",
    "            output : 유저-아이템 쌍에 대한 추천 결과 \n",
    "                ex) tensor([  9.4966,  22.0261, ..., -19.3535, -23.0212])\n",
    "        \"\"\"\n",
    "        user_embedding_mf = self.user_embedding_mf(user_indices)\n",
    "        item_embedding_mf = self.item_embedding_mf(item_indices)\n",
    "        mf_output = torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "        \n",
    "        user_embedding_mlp = self.user_embedding_mlp(user_indices)\n",
    "        item_embedding_mlp = self.item_embedding_mlp(item_indices)\n",
    "        genre_embedding_mlp = self.genre_embeddig(feats[1])\n",
    "        \n",
    "        keyword_embedding_mlp = self.keyword_embedding(feats[2])\n",
    "        if len(keyword_embedding_mlp.size()) == 3: keyword_embedding_mlp = keyword_embedding_mlp.mean(dim=-2)\n",
    "            \n",
    "        tag_embedding_mlp = self.tag_embedding(feats[3])\n",
    "        if len(tag_embedding_mlp.size()) == 3: tag_embedding_mlp = tag_embedding_mlp.mean(dim=-2)\n",
    "\n",
    "        input_feature = torch.cat((user_embedding_mlp, item_embedding_mlp, genre_embedding_mlp, keyword_embedding_mlp,\n",
    "                                   tag_embedding_mlp, feats[0].unsqueeze(1)), -1)\n",
    "        mlp_output = self.mlp_layers(input_feature)\n",
    "        \n",
    "        output = torch.cat([mlp_output, mf_output], dim=-1)\n",
    "        output = self.affine_output(output).squeeze(-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE7ZtudJYU1V"
   },
   "source": [
    "### 학습 및 추론 코드 구현\n",
    "\n",
    "- 학습 : Negative sampling을 활용하여 Binary Classification 진행 \n",
    "    - history 에 있는 album_id는 positive label로 그렇지 않은 album_id는 nagative label로 활용  \n",
    "    - 단, 이때 모든 album_id를 negative label로 활용하는 것이 아닌 일부만 사용 (neg_ratio 값에 따라서 개수 조정)\n",
    "- 추론 : 일부 데이터에 대해 recall, ndcg, coverage 성능 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGTiX3PsYU1V"
   },
   "source": [
    "#### 학습 및 추론에 필요한 데이터 셋 생성 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_UIdataset(train, neg_ratio):\n",
    "    \"\"\" 유저별 학습에 필요한 딕셔너리 데이터 생성 \n",
    "    Args:\n",
    "        train : 유저-아이템의 상호작용을 담은 행렬 \n",
    "            ex) \n",
    "                array([[0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        ...,\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.],\n",
    "                        [0., 0., 0., ..., 0., 0., 0.]])\n",
    "        neg_ratio : negative sampling 활용할 비율 \n",
    "            ex) 3 (positive label 1개당 negative label 3개)\n",
    "    Returns: \n",
    "        UIdataset : 유저별 학습에 필요한 정보를 담은 딕셔너리 \n",
    "            ex) {'사용자 ID': [[positive 샘플, negative 샘플], ... , [1, 1, 1, ..., 0, 0]]}\n",
    "                >>> UIdataset[3]\n",
    "                    [array([   16,    17,    18, ...,  9586, 18991,  9442]),\n",
    "                    array([5, 5, 5, ..., 5, 5, 5]),\n",
    "                    array([4, 4, 4, ..., 5, 1, 1]),\n",
    "                    array([1., 1., 1., ..., 0., 0., 0.])]\n",
    "    \"\"\"\n",
    "    UIdataset = {}\n",
    "    for user_id, items_by_user in enumerate(train):\n",
    "        UIdataset[user_id] = []\n",
    "        # positive 샘플 계산 \n",
    "        pos_item_ids = np.where(items_by_user > 0.5)[0]\n",
    "        num_pos_samples = len(pos_item_ids)\n",
    "\n",
    "        # negative 샘플 계산 (random negative sampling) \n",
    "        num_neg_samples = neg_ratio * num_pos_samples\n",
    "        neg_items = np.where(items_by_user < 0.5)[0]\n",
    "        neg_item_ids = np.random.choice(neg_items, min(num_neg_samples, len(neg_items)), replace=False)\n",
    "        UIdataset[user_id].append(np.concatenate([pos_item_ids, neg_item_ids]))\n",
    "        \n",
    "        # feature 추출 \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "            features.append(user_features['age'][user_id])\n",
    "        UIdataset[user_id].append(np.array(features))\n",
    "        \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]): \n",
    "            features.append(item_features['genre_mid'][item_id])\n",
    "        UIdataset[user_id].append(np.array(features))\n",
    "        \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]):\n",
    "            features.append(user_features['keyword'][user_id])\n",
    "        UIdataset[user_id].append(np.array(features))\n",
    "        \n",
    "        features = []\n",
    "        for item_id in np.concatenate([pos_item_ids, neg_item_ids]):\n",
    "            features.append(item_features['tag'][item_id])\n",
    "        UIdataset[user_id].append(np.array(features))\n",
    "        \n",
    "        # label 저장  \n",
    "        pos_labels = np.ones(len(pos_item_ids))\n",
    "        neg_labels = np.zeros(len(neg_item_ids))\n",
    "        UIdataset[user_id].append(np.concatenate([pos_labels, neg_labels]))\n",
    "\n",
    "    return UIdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UIdataset = make_UIdataset(train, neg_ratio=cfg.neg_ratio)\n",
    "UIdataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batchdata(user_indices, batch_idx, batch_size):\n",
    "    \"\"\" 배치 데이터로 변환 \n",
    "    Args:\n",
    "        user_indices : 전체 유저의 인덱스 정보 \n",
    "            ex) array([ 3100,  1800, 30098, ...,  2177, 11749, 20962])\n",
    "        batch_idx : 배치 인덱스 (몇번째 배치인지)\n",
    "            ex) 0 \n",
    "        batch_size : 배치 크기 \n",
    "            ex) 256 \n",
    "    Returns \n",
    "        batch_user_ids : 배치내의 유저 인덱스 정보 \n",
    "            ex) [22194, 22194, 22194, 22194, 22194, ...]\n",
    "        batch_item_ids : 배치내의 아이템 인덱스 정보 \n",
    "            ex) [36, 407, 612, 801, 1404, ...]\n",
    "        batch_feat0 : 배치내의 유저-아이템 인덱스 정보에 해당하는 feature0 정보 \n",
    "            ex) [6, 6, 6, 6, 6, ...]\n",
    "        batch_feat1 : 배치내의 유저-아이템 인덱스 정보에 해당하는 feature1 정보 \n",
    "            ex) [4,  4,  4, 23,  4, ...]\n",
    "        batch_labels : 배치내의 유저-아이템 인덱스 정보에 해당하는 label 정보 \n",
    "            ex) [1.0, 1.0, 1.0, 1.0, 1.0, ...]\n",
    "    \"\"\"\n",
    "    batch_user_indices = user_indices[batch_idx*batch_size : (batch_idx+1)*batch_size]\n",
    "    batch_user_ids = []\n",
    "    batch_item_ids = []\n",
    "    batch_feat0 = []\n",
    "    batch_feat1 = []\n",
    "    batch_feat2 = []\n",
    "    batch_feat3 = []\n",
    "    batch_labels = []\n",
    "    for user_id in batch_user_indices:\n",
    "        item_ids = UIdataset[user_id][0]\n",
    "        feat0 = UIdataset[user_id][1]\n",
    "        feat1 = UIdataset[user_id][2]\n",
    "        feat2 = UIdataset[user_id][3]\n",
    "        feat3 = UIdataset[user_id][4]\n",
    "        labels = UIdataset[user_id][5]\n",
    "        user_ids = np.full(len(item_ids), user_id)\n",
    "        batch_user_ids.extend(user_ids.tolist())\n",
    "        batch_item_ids.extend(item_ids.tolist())\n",
    "        batch_feat0.extend(feat0.tolist())\n",
    "        batch_feat1.extend(feat1.tolist())\n",
    "        batch_feat2.extend(feat2.tolist())\n",
    "        batch_feat3.extend(feat3.tolist())\n",
    "        batch_labels.extend(labels.tolist())\n",
    "    return batch_user_ids, batch_item_ids, batch_feat0, batch_feat1, batch_feat2, batch_feat3, batch_labels\n",
    "\n",
    "def update_avg(curr_avg, val, idx):\n",
    "    \"\"\" 현재 epoch 까지의 평균 값을 계산 \n",
    "    \"\"\"\n",
    "    return (curr_avg * idx + val) / (idx + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 및 검증 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS1EPRpeYU1W"
   },
   "outputs": [],
   "source": [
    "def train_epoch(cfg, model, optimizer, criterion): \n",
    "    model.train()\n",
    "    curr_loss_avg = 0.0\n",
    "\n",
    "    user_indices = np.arange(cfg.n_users)\n",
    "    np.random.RandomState(cfg.epoch).shuffle(user_indices)\n",
    "    batch_num = int(len(user_indices) / cfg.batch_size) + 1\n",
    "    bar = tqdm(range(batch_num), leave=False)\n",
    "    for step, batch_idx in enumerate(bar):\n",
    "        user_ids, item_ids, feat0, feat1, feat2, feat3, labels = make_batchdata(user_indices, batch_idx, cfg.batch_size)\n",
    "        \n",
    "        # 배치 사용자 단위로 학습\n",
    "        user_ids = torch.LongTensor(user_ids).to(cfg.device)\n",
    "        item_ids = torch.LongTensor(item_ids).to(cfg.device)\n",
    "        feat0 = torch.FloatTensor(feat0).to(cfg.device)\n",
    "        feat1 = torch.LongTensor(feat1).to(cfg.device)\n",
    "        feat2 = torch.LongTensor(feat2).to(cfg.device)\n",
    "        feat3 = torch.LongTensor(feat3).to(cfg.device)\n",
    "        labels = torch.FloatTensor(labels).to(cfg.device)\n",
    "        labels = labels.view(-1, 1)\n",
    "\n",
    "        # grad 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 forward\n",
    "        output = model.forward(user_ids, item_ids, [feat0, feat1, feat2])\n",
    "        output = output.view(-1, 1)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 최적화\n",
    "        optimizer.step()    \n",
    "        if torch.isnan(loss):\n",
    "            print('Loss NAN. Train finish.')\n",
    "            break\n",
    "        curr_loss_avg = update_avg(curr_loss_avg, loss, step)\n",
    "        \n",
    "        msg = f\"epoch: {cfg.epoch}, \"\n",
    "        msg += f\"loss: {curr_loss_avg.item():.5f}, \"\n",
    "        msg += f\"lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        bar.set_description(msg)\n",
    "    rets = {'losses': np.around(curr_loss_avg.item(), 5)}\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(actual, predicted, k = 25):\n",
    "    \"\"\" label과 prediction 사이의 recall 평가 함수 \n",
    "    Args:\n",
    "        actual : 실제로 본 상품 리스트\n",
    "        pred : 예측한 상품 리스트\n",
    "        k : 상위 몇개의 데이터를 볼지 (ex : k=5 상위 5개의 상품만 봄)\n",
    "    Returns: \n",
    "        recall_k : recall@k \n",
    "    \"\"\" \n",
    "    set_actual = set(actual)\n",
    "    recall_k = len(set_actual & set(predicted[:k])) / min(k, len(set_actual))\n",
    "    return recall_k\n",
    "\n",
    "def unique(sequence):\n",
    "    # preserves order\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def ndcgk(actual, predicted, k = 25):\n",
    "    set_actual = set(actual)\n",
    "    idcg = sum([1.0 / np.log(i + 2) for i in range(min(k, len(set_actual)))])\n",
    "    dcg = 0.0\n",
    "    unique_predicted = unique(predicted[:k])\n",
    "    for i, r in enumerate(unique_predicted):\n",
    "        if r in set_actual:\n",
    "            dcg += 1.0 / np.log(i + 2)\n",
    "    ndcg_k = dcg / idcg\n",
    "    return ndcg_k\n",
    "\n",
    "def evaluation(gt, pred):\n",
    "    \"\"\" label과 prediction 사이의 recall, coverage, competition metric 평가 함수 \n",
    "    Args:\n",
    "        gt : 데이터 프레임 형태의 정답 데이터 \n",
    "        pred : 데이터 프레임 형태의 예측 데이터 \n",
    "    Returns: \n",
    "        rets : recall, ndcg, coverage, competition metric 결과 \n",
    "            ex) {'recall': 0.123024, 'ndcg': 056809, 'coverage': 0.017455, 'score': 0.106470}\n",
    "    \"\"\"    \n",
    "    gt = gt.groupby('profile_id')['album_id'].unique().to_frame().reset_index()\n",
    "    gt.columns = ['profile_id', 'actual_list']\n",
    "\n",
    "    evaluated_data = pd.merge(pred, gt, how = 'left', on = 'profile_id')\n",
    "\n",
    "    evaluated_data['Recall@25'] = evaluated_data.apply(lambda x: recallk(x.actual_list, x.predicted_list), axis=1)\n",
    "    evaluated_data['NDCG@25'] = evaluated_data.apply(lambda x: ndcgk(x.actual_list, x.predicted_list), axis=1)\n",
    "\n",
    "    recall = evaluated_data['Recall@25'].mean()\n",
    "    ndcg = evaluated_data['NDCG@25'] .mean()\n",
    "    coverage = (evaluated_data['predicted_list'].apply(lambda x: x[:cfg.top_k]).explode().nunique())/meta_df.index.nunique()\n",
    "\n",
    "    score = 0.75*recall + 0.25*ndcg\n",
    "    rets = {\"recall\" :recall, \n",
    "            \"ndcg\" :ndcg, \n",
    "            \"coverage\" :coverage, \n",
    "            \"score\" :score}\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQgUJbTOYU1X"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(cfg, model, data, mode='valid'):\n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    query_user_ids = data['profile_id'].unique() # 추론할 모든 user array 집합\n",
    "    full_item_ids = np.array([c for c in range(cfg.n_items)]) # 추론할 모든 item array 집합 \n",
    "    full_item_ids_feat1 = [item_features['genre_mid'][c] for c in full_item_ids]\n",
    "    full_item_ids_feat3 = [item_features['tag'][c] for c in full_item_ids]\n",
    "    for user_id in query_user_ids:\n",
    "        with torch.no_grad():\n",
    "            user_ids = np.full(cfg.n_items, user_id)\n",
    "            \n",
    "            user_ids = torch.LongTensor(user_ids).to(cfg.device)\n",
    "            item_ids = torch.LongTensor(full_item_ids).to(cfg.device)\n",
    "            \n",
    "            feat0 = np.full(cfg.n_items, user_features['age'][user_id])\n",
    "            feat0 = torch.FloatTensor(feat0).to(cfg.device)\n",
    "            feat1 = torch.LongTensor(full_item_ids_feat1).to(cfg.device)\n",
    "            \n",
    "            feat2 = np.empty((cfg.n_items, cfg.n_keywords))\n",
    "            keyword_feat = user_features['keyword'][user_id]\n",
    "            for i in range(len(feat2)): feat2[i] = keyword_feat\n",
    "            feat2 = torch.LongTensor(feat2).to(cfg.device)\n",
    "            \n",
    "            feat3 = torch.LongTensor(full_item_ids_feat3).to(cfg.device)\n",
    "            \n",
    "            eval_output = model.forward(user_ids, item_ids, [feat0, feat1, feat2, feat3]).detach().cpu().numpy()\n",
    "            pred_u_score = eval_output.reshape(-1)   \n",
    "        \n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = full_item_ids[pred_u_idx]\n",
    "        pred_list.append(list(pred_u[:cfg.top_k]))\n",
    "        \n",
    "    pred = pd.DataFrame()\n",
    "    pred['profile_id'] = query_user_ids\n",
    "    pred['predicted_list'] = pred_list\n",
    "    \n",
    "    # 모델 성능 확인 \n",
    "    if mode == 'valid':\n",
    "        rets = evaluation(data, pred)\n",
    "        return rets, pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-D5bJx7YU1X"
   },
   "source": [
    "### 하이퍼파라미터 설정 & 최적화 기법 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정 \n",
    "cfg.batch_size = 512\n",
    "cfg.emb_dim = 256\n",
    "cfg.layer_dim = 256\n",
    "cfg.dropout = 0.05\n",
    "cfg.epochs = 15\n",
    "cfg.learning_rate = 0.0025\n",
    "cfg.reg_lambda = 0\n",
    "cfg.check_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model 생성 및 optimizer, loss 함수 설정 \n",
    "model = NeuMF(cfg).to(cfg.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.reg_lambda)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eze0e7vtYU1Y"
   },
   "source": [
    "### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvhj3hD7YU1Y"
   },
   "outputs": [],
   "source": [
    "total_logs = defaultdict(list)\n",
    "best_scores  = 0\n",
    "for epoch in range(cfg.epochs+1):\n",
    "    cfg.epoch = epoch\n",
    "    train_results = train_epoch(cfg, model, optimizer, criterion)\n",
    "    \n",
    "    # cfg.check_epoch 번의 epoch 마다 성능 확인 \n",
    "    if epoch % cfg.check_epoch == 0: \n",
    "        valid_results, _ = valid_epoch(cfg, model, valid)\n",
    "\n",
    "        logs = {\n",
    "            'Train Loss': train_results['losses'],\n",
    "            f'Valid Recall@{cfg.top_k}': valid_results['recall'],\n",
    "            f'Valid NDCG@{cfg.top_k}': valid_results['ndcg'],\n",
    "            'Valid Coverage': valid_results['coverage'],\n",
    "            'Valid Score': valid_results['score'],\n",
    "            }\n",
    "\n",
    "        # 검증 성능 확인 \n",
    "        for key, value in logs.items():\n",
    "            total_logs[key].append(value)\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(\"Epoch\", end=\",\")\n",
    "            print(\",\".join(logs.keys()))\n",
    "\n",
    "        print(f\"{epoch:02d}  \", end=\"\")\n",
    "        print(\"  \".join([f\"{v:0.6f}\" for v in logs.values()]))\n",
    "        \n",
    "        # 가장 성능이 좋은 가중치 파일을 저장 \n",
    "        if best_scores <= valid_results['score']: \n",
    "            best_scores = valid_results['score']\n",
    "            torch.save(model.state_dict(), os.path.join(saved_path, 'model(best_scores).pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNVpn37RYU1Z"
   },
   "source": [
    "### 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = pd.DataFrame({'Train loss': total_logs['Train Loss']})\n",
    "train_scores['Epoch'] = range(0, cfg.epochs+1, cfg.check_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(train_scores, aes(x='Epoch', y='Train loss'))\n",
    "        + geom_line(color='black') # line plot\n",
    "        + labs(x='Epoch', y='Train Loss')\n",
    "        + theme_light()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores = pd.DataFrame(np.hstack([(range(0, cfg.epochs+1, cfg.check_epoch), total_logs[score], [score for i in range(0, cfg.epochs+1, cfg.check_epoch)]) for score in ['Valid Recall@25', 'Valid NDCG@25', 'Valid Coverage', 'Valid Score']])).T\n",
    "valid_scores.columns = ['Epoch', 'Score', 'Metric']\n",
    "valid_scores['Epoch'] = valid_scores['Epoch'].astype(int)\n",
    "valid_scores['Score'] = valid_scores['Score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "ggplot(valid_scores)  # here\n",
    "    + aes(\"Epoch\", \"Score\", color='Metric', group='Metric')\n",
    "    + geom_line()\n",
    "    + scale_y_continuous(breaks=[0.1*c for c in range(1, 10, 1)])\n",
    "    + theme_light()\n",
    "    + labs(x='Epoch', y='Valid Metric')\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07lHW5CAYU1F"
   },
   "source": [
    "## 제출 \n",
    "### 모든 유저에 대해 추천 결과 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(saved_path, 'model(best_scores).pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission_path = os.path.join(data_path, 'sample_submission.csv')\n",
    "submission = pd.read_csv(submission_path)\n",
    "submission = valid_epoch(cfg, model, submission, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(output_path, 'agkt_submission.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "기본과제-2_NCF+AutoRec (answer).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1dbd43fe9b6ac5035e713bc64837ed61b2c8b42edd7497e359f007ee89da5e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
